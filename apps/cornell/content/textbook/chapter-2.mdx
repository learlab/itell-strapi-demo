---
title: "Emotional Learning Analytics"
page_slug: emotional
summary: true
quiz: false
---
<div className="content-chunk" data-subsection-id = "Abstract-98pt">
## Abstract
This chapter discusses the ubiquity and importance of emotion to learning. It argues substantial progress can be made by coupling discovery-oriented, data-driven, analytic methods of learning analytics and educational data mining with theoretical advances and methodologies from the affective and learning sciences. Core, emerging, and future themes of research at the intersection of these areas are discussed.

**Keywords**: Affect, affective science, affective computing, educational data mining, learning analytics
</div>
<div className="content-chunk" data-subsection-id = "Introduction-1-476t">
At the recommendation of a reviewer of one of my papers \[15\], I recently sought to learn a statistical method called generalized additive mixed models (GAMMs) \[48\]. At first, I was mildly displeased at the thought of having to do more work on this paper. I downloaded a recommended paper with some eye-catching graphics, which piqued my curiosity and motivated me to explore further. This quickly turned into interest as I read more, and eventually into excitement when I realized the power of the approach. This motivated me to slog through the technical details, which led to confusion and frustration when things did not make sense, and delight when I made progress. When I attempted to apply the method to my data, I felt more confusion and frustration, interspersed with hope, delight, and happiness. I eventually got it working and wrote the results. When I was done, I felt contentment, relief, and a bit of pride.

As this example illustrates, emotion pervades the learning process. This is not unique to learning as much cognition is tinged with emotion. Emotions are not always consciously experienced \[54\], but they exist and influence cognition nonetheless. Also, emotions do not occur in a vacuum; they are deeply intertwined within the social fabric of learning. Students experience a range of emotions during learning. Pekrun & Stephens \[57\] call these “academic emotions” and group them into four categories. Achievement emotions are linked to learning activities (homework, taking a test) and outcomes (success, failure). Topic emotions are aligned with the learning content (empathy for a protagonist). Social emotions such as pride, shame, and jealousy occur because education requires interacting with others. Finally, epistemic emotions arise in the course of cognitive processing, such as confusion in the face of an impasse.

Emotions are not merely incidental; they may have evolved to serve specific functions \[23, 69\]. For example, emotions perform signaling functions \[66\] by highlighting problems with knowledge (confusion), problems with stimulation (boredom), concerns with impending performance (anxiety), and challenges not easily surpassed (frustration). They perform evaluative functions by serving as the currency by which people appraise an event in terms of its value, goal relevance, and goal congruence \[38\]. Emotions perform modulation functions by changing cognitive focus; negative emotions engender narrow, bottomup, and focused processing \[8, 66\] compared to positive emotions, which facilitate broader, top-down, generative processing \[29, 37\]. Emotions pervade thought through their effects on memory, problem solving, decision making, and other facets of cognition (see \[12\] for a review).
</div>
<div className="content-chunk" data-subsection-id = "Introduction-2-478t">
What exactly is an emotion? Truth be told, we do not really know, or at least we do not fully agree \[38\]. This can be readily inferred from recent debates on the psychological underpinnings of emotion. Fortunately, there is general agreement on the following key points. Emotions are conceptual or experienced entities arising from brain–body–environment interactions. However, you won’t find them by looking in the brain, body, or environment. Instead, emotions emerge \[46\] when organism–environment interactions trigger changes across multiple time scales and at multiple levels—neurobiological, physiological, behavioral, and subjective. The emotion is reflected in these changes in a manner modulated by previous experience and the ongoing situational context. The same emotional category (e.g., anxiety) will manifest differently based on a triggering event \[69\], the specific biological/cognitive/metacognitive processes involved \[33, 50\], and sociocultural influences \[49, 56\]. For example, an anxiety-inducing event will trigger distinct “episodes” of anxiety depending on the specific circumstance (public speaking, test taking), the temporal context (one day versus one minute before the speech), the neurobiological system (baseline arousal), and the social context (speaking in front of colleagues versus strangers). This level of variability and ambiguity is expected because humans and their emotions are dynamic and adaptive. Rigid emotions have little evolutionary value as our environment is always changing.
</div>
<div className="content-chunk" data-subsection-id = "Introduction-3-477t">
Where do learning analytics (LA) and educational data mining (EDM) fit in? On one hand, given the key role of emotions in learning, attempts to analyze learning without considering emotion will be incomplete. On the other hand, given the ambiguity and complexity of emotional phenomena, attempts to study emotions during learning without the methods of LA and EDM will only yield shallow insights. There is a body of work adopting a data-driven analytic approach to study the incidence and influence of emotions on the processes and products of learning. In this chapter, we highlight some of the core, emerging, and future themes in this interdisciplinary research area.

First, a note on terminology. Emotion is related but not equivalent to motivation, attitudes, preferences, physiology, arousal, and a host of other constructs. Emotions are also distinct from moods and affective traits \[62\]. Emotion is not the same as a feeling. Hunger is a feeling but is not an emotion. There is some contention as to what constitutes an emotion. Anger is certainly an emotion, but what about confusion? Confusion has affective components (feelings of being confused, characteristic facial expressions; \[18\], but there is debate as to whether it is an emotion \[34, 63\]. In the remainder of this chapter, we use the more inclusive term affective state rather than the more restrictive term emotion.
</div>
<div className="content-chunk" data-subsection-id = "Core-Themes-99pt">
## Core Themes
We selected the following themes to highlight the use of LA/EDM methods to study affect during learning. We also deeply review a few exemplary studies within each theme rather than cursorily reviewing many studies. This means many excellent studies go unmentioned, but we leave it to the reader to explore the body of work within each theme. When available, we recommend review papers to facilitate this process.
</div>
<div className="content-chunk" data-subsection-id = "01-Affect-Detection-from-Student-Activity-Data-480t">
## 0.1 Affect Detection from Student Activity Data
Affective states cannot be directly measured because they are conceptual entities. Because they emerge from environment–person interactions and influence action by modulating cognition, it should be possible to infer affect by analyzing context and learner actions. This approach, referred to as “interaction-based,” “log-file based,” or “sensor-free” affect detection has a decade-long history \[1, 16\](and was recently reviewed by \[7\]).

As an example, Pardos, Baker, San Pedro, Gowda, & Gowda \[55\] developed affect detectors for ASSISTments, an intelligent tutoring system (ITS) for middle- and highschool mathematics \[60\]. The authors collected training data from 229 students while they used ASSISTments in school computer labs. Human observers recorded affect as students interacted with ASSISTments using the Baker-Rodrigo Observation Method Protocol (BROMP) \[52\]. According to this protocol, trained observers make live annotations of affect based on observable behavior, including explicit actions towards the software’s interface, interactions with peers and teachers, body movements, gestures, and facial expressions. The observers coded four affective states (boredom, frustration, engaged concentration, and confusion) and two behaviors (going off-task and gaming the system1). Supervised learning techniques were used to discriminate each affective state from other states (e.g., bored versus others) using features extracted from ASSISTments log files (performance on problems, hint requests, response times). Accuracy for detecting affective states ranged from .632 to .678 (measured with the A-prime metric, similar to AUROC) for affect and .802 to .819 for behaviors. The classifier was validated in a manner ensuring generalizability to new students from the same population by ensuring each student’s data appears only in the training or the testing data. Pardos et al. \[55\] provided preliminary evidence on the predictive validity of their detectors. This was done by applying the detectors on log files from a different set of 1,393 students who interacted with ASSISTments several years earlier. Automatically measured affect and behavior moderately correlated with standardized test scores (0:09 < |r| < 0:45).
</div>
<div className="content-chunk" data-subsection-id = "Core-Themes-2-479t">
Further, San Pedro, Baker, Bowers, & Heffernan \[65\] attempted to predict college enrollment based on the automatic detectors. They applied the detectors to existing log files from 3,707 students who interacted with ASSISTments from 2004 to 2009. College enrollment information for these students was obtained from the National Student Clearinghouse. Automatically measured affective states were significant predictors of college enrollment several years later, which is a rather impressive finding.

More recently, Hutt, Grafsgaard, & D’Mello \[36\] developed a sensor-free measure of student engagement with an eye towards scalability. The research was conducted in the context of Algebra Nation, an online math learning platform supporting over 150,000 diverse students studying Algebra 1, Algebra 2, and Geometry. The researchers collected a large-scale dataset of 69,174 students who used Algebra Nation as part of their regular math classes for a semester and used experience sampling to collect 133,966 self-reports of 18 affective states (e.g., boredom, confusion, mind wandering, curiosity) related to engagement. They identified 22 generic activity features (viewing a video, pausing a video, taking a quiz) extracted from Algebra Nation log files in 5-minute windows prior to a self-report survey. These features do not require specialized sensors and are domain- and system-independent. They trained supervised learning models to predict each affective state from the features. Prediction accuracies (Spearman’s rho, a correlation coefficient ranging from -1 to 1), were modest and ranged from .08 (for surprise) to .34 (for happiness), with a mean of .25.

The researchers tested the generalizability of the engagement models in several ways. First, they showed the models trained on Algebra students generalized to a different data set of Geometry students (n = 28,458) on the same platform. Jensen, Hutt, & D’Mello \[41\] demonstrated the models’ generalizability to clusters of students based on typical platform use and demographic features. They found models trained on one group performed similarly well when tested on the other groups, although there was a small advantage of training multiple individual subpopulation models compared to a general (all-population) model. These results show the promise of scaling up sensor-free methods to detect engagement on the largest and most heterogeneous student sample to date.
</div>
<div className="content-chunk" data-subsection-id = "02-Affect-Detection-from-Bodily-Signals-481t">
## 0.2 Affect Detection from Bodily Signals
Affect is an embodied phenomenon in that it activates bodily response systems for action. Signals of these bodily responses should make it possible to infer learner affect (a latent variable) from machine-readable bodily signals (observables). There is a rich body of work on the use of bodily signals to detect affect as discussed in a number of reviews \[11, 20, 75\]. Research has historically focused on interactions in controlled environments, but researchers have begun to take this work into the real world, specifically computer-enabled classrooms. The study reviewed next reflects one such effort by our research group and collaborators, but the reader is directed to Arroyo et al. \[2\] for their pioneering work on affect detection in computerenabled classrooms.

Bosch, D’Mello, Ocumpaugh, Baker, & Shute \[10\] studied automated detection of affect from facial features in a noisy real-world setting of a computer-enabled classroom. In this study, 137 middle and high school students played a conceptual physics educational game called Physics Playground \[67\] in small groups for 1.5 to 2 hours across two days as part of their regular physics classes. Trained observers made live annotations of boredom, confusion, frustration, engaged concentration, and delight using the BROMP protocol. The observers also noted when students went off-task.

Videos of students’ faces and upper bodies were recorded during game-play and synchronized with the affect annotations. The videos were processed using the FACET computer-vision program \[26\], which provides estimates of the likelihood of 19 facial action units \[25\] (e.g., raised brow, tightened lips), head pose, and position (Figure 1). Body movement was also estimated from the videos using motion filtering algorithms \[44\] (Figure 2). Supervised learning methods were then used to build detectors of each affective state (e.g., bored vs. other states) using both facial expressions and bodily movements. The detectors were moderately successful with accuracies (quantified by AUROC) ranging from .610 to .867 for affect and .816 for off-task behaviors. Follow-up analyses confirmed the affect detectors generalized across students, multiple days, class periods, and across different genders and ethnicities.
</div>
<div className="content-chunk" data-subsection-id = "Core-Themes-3-483t">
One limitation of the face-based affect detectors is they are only applicable when the face can be automatically detected in the video stream. This is not always the case due to excessive movement, occlusion, poor lighting, and other factors. In fact, the face-based affect detectors were only applicable to 65% of the cases. To address this, Bosch, Chen, D’Mello, Baker, & Shute \[9\] combined interactionbased and face-based detection via decision-level fusion. The interaction-based detectors were less accurate than the face-based detectors (Kai et al., \[42\]), but were applicable to almost all of the cases. By combining the two, the detectors could be applied to 98% of the cases, with only a small reduction (less than 5% difference) in accuracy compared to face-based detection.

![chapter12-figure1.png](https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/chapter12-figure1.png-b381b6bc8c2b162e66a837e8c89530e6.png)![figure 2.png](https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/figure%202.png-d36f708a0737de85fca32ef0a1502ec8.png)
</div>
<div className="content-chunk" data-subsection-id = "03-Integrating-Affect-Models-in-Affect-Aware-Learning-Technologies-484t">
## 0.3 Integrating Affect Models in Affect-Aware Learning Technologies
The interaction- and bodily-based affect detectors just discussed are tangible artifacts that can be instrumented to provide real-time assessments of student affect during interactions with a learning technology. This affords the exciting possibility of closing the loop by dynamically responding to the sensed affect. The aim of such affectaware learning technologies is to expand the bandwidth of adaptivity of current learning technologies by responding to what students feel in addition to what they think and do (see \[22\] for a review). Here, we highlight two such systems, the Affective AutoTutor \[17\] and UNC-ITSPOKE \[28\].

Affective AutoTutor (see Figure 3) is a modified version of AutoTutor — a conversational ITS that helps students develop mastery on difficult topics in Newtonian physics, computer literacy, and scientific reasoning by holding a mixed-initiative dialog in natural language \[31\]. The original AutoTutor system has a set of fuzzy production rules sensitive to the cognitive states of the learner. The Affective AutoTutor augments these rules to be sensitive to assessments of learners’ changing affective states, specifically boredom, confusion, and frustration. The affective states are sensed by automatically monitoring interaction patterns, gross body movements, and facial features \[17\]. The Affective AutoTutor responds with empathetic, encouraging, and motivational dialog-moves along with an avatar’s emotional displays. For example, the tutor might respond to mild boredom with, “This stuff can be kind of dull sometimes, so I’m gonna try and help you get through it. Let’s go.” The affective responses are accompanied by appropriate emotional facial expressions and emotionally modulated speech (e.g., synthesized empathy or encouragement).
</div>
<div className="content-chunk" data-subsection-id = "Core-Themes-4-485t">
![figure3.png](https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/figure3.png-6c500fd237858610f1cdd6df31df6bfd.png)

The effectiveness of Affective AutoTutor over the original non-affective AutoTutor was tested in a between-subjects experiment where 84 learners were randomly assigned to two 30-minute learning sessions with an affective-aware or non-affective tutor \[21\]. The results indicated the affective tutor helped learning for low-domain knowledge learners during the second 30-minute learning session. The affective tutor was less effective at promoting learning for high-domain knowledge learners and during the first 30-minute session. Importantly, learning gains increased from Session 1 to Session 2 with the affective tutor whereas they plateaued with the non-affective tutor. Learners who interacted with the affective tutor also demonstrated improved performance on a subsequent transfer test. A follow-up analysis indicated learners’ perceptions of how closely the computer tutors resembled human tutors increased across learning sessions, related to the quality of tutor feedback, and powerfully predicted learning \[19\]. The positive change in perceptions was greater for the affective tutor.

As a second example, consider UNC-ITSPOKE \[28\], a speech-enabled ITS for physics which automatically detects and responds to learners’ certainty/uncertainty in addition to the correctness/incorrectness of their spoken responses. Uncertainty detection was performed by extracting and analyzing acoustic-prosodic features in learners’ spoken responses along with lexical and dialog-based features. UNC-ITSPOKE responded to uncertainty when the learner was correct but uncertain about the response. The response strategy involved launching explanationbased sub-dialogs that provided added instruction to remediate the uncertainty. This could involve additional follow-up questions (for more difficult content) or simply the assertion of the correct information with elaborated explanations (for easier content).

Forbes-Riley & Litman \[28\] compared learning outcomes of 72 learners who were randomly assigned to receive adaptive responses to uncertainty (adaptive condition), no responses to uncertainty (non-adaptive control condition), or random responses to uncertainty (random control condition). In this later condition, the added tutorial content from the sub-dialogs was given for a random set of turns to control for the additional tutoring. The results indicated the adaptive condition achieved slightly (but not significantly) higher learning outcomes than the other conditions. The findings revealed it was perhaps not the presence or absence of adaptive responses to uncertainty, but the number of adaptive responses that correlated with learning outcomes.
</div>
<div className="content-chunk" data-subsection-id = "1-Emerging-Themes-482t">
## 1 Emerging Themes
Research at the intersection of emotions, learning, LA, and EDM, has typically focused on one-on-one learning with an ITS \[28, 32, 73\], educational games \[13, 43, 64\], or interfaces that support basic competencies like reading and problem solving \[21, 45\]. Although these basic lines of research are quite active, recent work has focused on analyzing affect across interaction contexts that more closely reflect the broader sociocultural context surrounding learning. We briefly describe four themes of research to illustrate a few exciting developments.

### 1.1 Affect-Based Predictors of Engagement and Dropout

Indicators of potential dropout or poor grades and corresponding early intervention systems are some of the “killer apps” of LA and EDM \[39\]. Most systems in authentic settings focus on academic performance data, demographics, and availability of financial assistance. These factors are undoubtedly important, but there are likely alternate factors that come into play. With this in mind, Tze, Daniels, Buhr, & Le \[70\] identified affective profiles in a Massive Open Online Course (MOOC). They found these different profiles were associated with varying levels of cognitive, behavioral, and social engagement with the course. For example, profiles with lower levels of boredom and guilt were associated with higher engagement with course content and profiles with high anxiety were associated with higher social engagement. Additionally, Dillon et al. \[24\] analyzed frequently occurring affective states in a MOOC and found states such as anxiety, confusion, frustration, and hope were positively associated with dropout.

### 1.2 Sentiment Analysis of Discussion Forums

Language often communicates feelings. Hence, sentiment analysis and opinion mining techniques (Pang & Lee, 2008) have considerable potential for studying how students’ thoughts about a learning experience predict relevant behaviors like attrition. In line with this, Wen, Yang, & Rosé \[72\], applied sentiment analysis techniques to student posts on three MOOCs. They observed a negative correlation between dropout and the ratio of positive to negative posts. More recently, Xing, Tang, & Pei \[74\] expanded this analysis to specific academic achievement emotions. They found a student’s exposure to classmates’ negative emotions in discussions was the best predictor of future dropout.

### 1.3 Classroom Analytics

Recent advances in sensing and signal processing technologies have made it possible to automatically model aspects of students’ classroom experience that could previously only be obtained from self-reports and labor intensive human observations (such as BROMP). Hutt, Krasich, et al. \[35\] used eye-gaze features to predict mind wandering when high-school students used a biology ITS in their regular classroom. Models using eye-gaze data were incorporated into the tutoring system to provide real-time mind wandering estimates for evaluation and to drive interventions. On a different scale, Ramakrishnan, Ottmar, Locasale-crouch, & Whitehill \[59\] used classroom audio and video to automatically identify positive and negative classroom climate. Finally, Aslan et al. \[3\] developed a dashboard to alert teachers to student real-time behavioral (on- or off-task) and emotional (bored, satisfied, confused) engagement (Figure 4). By using the interface, teachers could focus on addressing student comprehension rather than discipline; additionally, students showed less increase in boredom over the semester. These analytics can then be used by teachers or students to improve engagement in the classroom, such as reviewing a topic when confusion is detected or redirecting focus when mind wandering occurs.
</div>
<div className="content-chunk" data-subsection-id = "14-Teacher-Analytics-486t">
## 1.4 Teacher Analytics
Teachers should not be left out of the loop since their practices influence student affect and engagement. Unfortunately, quantifying teacher instructional practices relies on live observations in classrooms (e.g., Nystrand, Gamoran, Kachur, & Prendergast \[51\]), which makes the research difficult to scale. To address this, researchers are developing methods for automatic analysis of teacher instructional practices. In a pioneering study, Wang, Miller, & Cortina \[71\] recorded classroom audio in 1st to 3rd grade math classes and developed automatic methods to predict the level of discussions in these classes. This work was recently expanded to analyze specific discourse features in larger samples of middle-school classes in literature and language-arts. Jensen et al. \[40\] analyzed self-recorded teacher audio to automatically detect seven discourse features (e.g., asking questions, providing feedback), achieving a modest correlation with human-coded labels and demonstrating a robustness to audio quality changes. The next step in this line of work will be to use information on what teachers are doing to contextualize how students are feeling, which in turn influences what the students think, do, and learn.

<Image
  style="aspect-ratio:582/568;"
  src="https://nbjrajrmujlgxmcvqsge.supabase.co/storage/v1/object/public/strapi/files/figure4.png-4e78eb7f6a8b305d6f26b32b6ed17195.png"
  alt="figure4.png"
  width="582"
  height="568">
 
</Image>
</div>
<div className="content-chunk" data-subsection-id = "2-Future-Themes-487t">
## 2 Future Themes
Let us end by briefly highlighting some potential themes of future research. One promising area of research involves a detailed analysis of the emotional experience of learners and communities of learners across the extended time \[35\]. A second involves the study of emotion regulation during learning, especially whether LA/EDM methods can be used to identify different regulatory strategies \[33\], and encourage more beneficial ones (e.g., \[5, 58, 68\]). A third would jointly consider how emotion arises and shifts alongside attentional states of mindfulness, mind wandering, and flow \[14\]. A fourth addresses how “noncognitive” \[27\] traits like grit, self-control, and diligence modulate learner emotions and regulation (e.g., \[30, 47\]). A fifth would monitor the occurrence and interaction of emotions of individual learners and the team as a whole during collaborative learning and collaborative problem solving \[4, 61\] given the importance of collaboration as a critical 21st century skill \[53\].

Developments in these themes could then be applied to develop interventions that aim to make the learning experience more engaging and effective. This could take the form of redirecting attention, providing tools for emotion regulation, or adjusting instruction to meet student needs (i.e., scaffolding). An important challenge is developing “fail-soft” interventions; that is, if the analysis of a student’s current affective state is incorrect, then the resulting intervention will not negatively impact them.

Research to date has mainly focused on the achievement, epistemic, and topic emotions. However, an analysis of learning situated within sociocultural contexts must address the social emotions such as pride, shame, guilt, jealousy, and envy.
</div>
<div className="content-chunk" data-subsection-id = "3-Conclusion-488t">
## 3 Conclusion
Learning is not a cold intellectual activity; it is nuanced with emotion. Emotions are not merely decorative, they influence our thoughts and behavior. However, emotion is a complex phenomenon with multiple components that dynamically unfold. Despite great strides in the fields of affective sciences and affective neuroscience, we need to know more about emotions, and more about emotions during learning. This does not imply we should refrain from modelling emotion until there is more theoretical clarity; we instead need to be mindful of what we are modelling when we say we are modelling emotion. We also need to embrace, rather than dilute, the complexity and ambiguity inherent in emotion. If anything, the discovery-oriented, data-driven, analytic methods of LA and EDM, especially when applied to data gathered in real-world settings, has the unique potential to advance both the science of learning and the science of emotion. It all begins by incorporating emotion into the analysis of learning.
</div>
<div className="content-chunk" data-subsection-id = "Acknowledgements-100pt">
## Acknowledgements
This research was supported by the National Science Foundation (IIS 1735793), Intel Research, and the Institute for Educational Sciences (UFDSP00011829). Any opinions, findings, conclusions, or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.
</div>
<div className="content-chunk" data-subsection-id = "References-101pt">
<Info title="References">\[1\] Hua Ai, Diane J. Litman, Kate Forbes-Riley, Mihai Rotaru, Joel Tetreault, and Amruta Purandare. “Using System and User Performance Features to Improve Emotion Detection in Spoken Tutoring Dialogs”. In: Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH 2 (2006), pp. 797–800. ISSN: 9781604234497.  \[2\] Ivon Arroyo, David G. Cooper, Winslow Burleson, Beverly Park Woolf, Kasia Muldner, and Robert Christopherson. “Emotion Sensors Go to School”. In: Frontiers in Artificial Intelligence and Applications 200.1 (2009), pp. 17–24. ISSN: 9781607500285. DOI: 10.3233/978-1-60750-028-5-17. \[3\] Sinem Aslan, Nese Alyuz, Cagri Tanriover, Sinem E. Mete, Eda Okur, Sidney K. D’Mello, and Asli Arslan Esme. “Investigating the Impact of a Real-Time, Multimodal Student Engagement Analytics Technology in Authentic Classrooms”. In: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems - CHI ’19. New York, New York, USA: ACM Press, 2019, pp. 1–12. ISBN: 978-1-4503-5970-2. DOI: 10.1145/3290605.3300534. \[4\] Sunny Avry, Guillaume Chanel, Mireille Bétrancourt, and Gaëlle Molinari. “Achievement Appraisals, Emotions and Socio-Cognitive Processes: How They Interplay in Collaborative ProblemSolving?” In: Computers in Human Behavior (Jan. 2020). DOI: 10.1016/j.chb.2020.106267. \[5\] Roger Azevedo and Robert M. Bernard. “A MetaAnalysis of the Effects of Feedback in ComputerBased Instruction”. In: Journal of Educational Computing Research 13.2 (1995), pp. 111–127. DOI: 10. 2190/9lmd-3u28-3a0g-ftqt. \[6\] Ryan S.J.d. Baker, Albert T. Corbett, Kenneth R. Koedinger, Shelley Evenson, Ido Roll, Angela Z. Wagner, Meghan Naim, Jay Raspat, Daniel J. Baker, and Joseph E. Beck. “Adapting to When Students Game an Intelligent Tutoring System”. In: Intelligent Tutoring Systems. Ed. by Mitsuru Ikeda, Kevin D Ashley, and Tak-Wai Chan. Berlin: Springer Berlin Heidelberg, 2006, pp. 392–401. ISBN: 978-3-540- 35160-3. \[7\] Ryan S.J.d. Baker and Jaclyn Ocumpaugh. “Interaction-Based Affect Detection in Educational Software”. In: The Oxford Handbook of Affective Computing. Ed. by Rafael A. Calvo, Sidney K. D’Mello, Jonathan Gratch, and Arvid Kappas. Oxford University Press, Jan. 2015. ISBN: 978-0-19-994223-7. DOI: 10 . 1093 / oxfordhb / 9780199942237 . 013.009. \[8\] Carola M. Barth and Joachim Funke. “Negative Affective Environments Improve Complex Solving Performance”. In: Cognition & Emotion 24.7 (Nov. 2010), pp. 1259–1268. DOI: 10 . 1080 / 02699930903223766. \[9\] Nigel Bosch, Huili Chen, Sidney K. D’Mello, Ryan S.J.d. Baker, and Valerie J. Shute. “Accuracy vs. Availability Heuristic in Multimodal Affect Detection in the Wild”. In: Proceedings of the 2015 ACM on International Conference on Multimodal Interaction - ICMI ’15. New York: ACM Press, 2015, pp. 267–274. ISBN: 978-1-4503-3912-4. DOI: 10.1145/2818346. 2820739. \[10\] Nigel Bosch, Sidney K. D’Mello, Jaclyn Ocumpaugh, Ryan S.J.d. Baker, and Valerie Shute. “Using Video to Automatically Detect Learner Affect in Computer-Enabled Classrooms”. In: ACM Transactions on Interactive Intelligent Systems 6.2 (July 2016), pp. 1–26. DOI: 10.1145/2946837. \[11\] Rafael A. Calvo and Sidney K. D’Mello. “Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications”. In: IEEE Transactions on Affective Computing 1.1 (2010), pp. 18–37. ISSN: 1949-3045 VO - 1. DOI: 10.1109/T-AFFC. 2010.1. \[12\] Gerald L. Clore and Jeffrey R. Huntsinger. “How Emotions Inform Judgment and Regulate Thought”. In: Trends in Cognitive Sciences 11.9 (Sept. 2007), pp. 393–399. DOI: 10.1016/j.tics.2007.08. 005. \[13\] Cristina Conati and Heather Maclaren. “Empirically Building and Evaluating a Probabilistic Model of User Affect”. In: User Modeling and User-Adapted Interaction 19.3 (Aug. 2009), pp. 267–303. DOI: 10. 1007/s11257-009-9062-8. \[14\] Mihaly Csikszentmihalyi. Flow: The Psychology of Optimal Experience. New York: Harper & Row, 1990. \[15\] Sidney K. D’Mello. “On the Influence of an Iterative Affect Annotation Approach on Inter-Observer and Self-Observer Reliability”. In: IEEE Transactions on Affective Computing 7.2 (Apr. 2016), pp. 136–149. DOI: 10.1109/TAFFC.2015.2457413. \[16\] Sidney K. D’Mello, Scotty Craig, Jeremiah Sullins, and Arthur C. Graesser. “Predicting Affective States Expressed through an Emote-Aloud Procedure from Autotutor’s Mixed-Initiative Dialogue”. In: International Journal of Artificial Intelligence in Education 16.1 (2006), pp. 3–28. ISSN: 1560-4292. \[17\] Sidney K. D’Mello and Arthur C. Graesser. “AutoTutor and Affective Autotutor: Learning by Talking with Cognitively and Emotionally Intelligent Computers That Talk Back”. In: ACM Transactions on Interactive Intelligent Systems 2.4 (Dec. 2012), pp. 1–39. DOI: 10.1145/2395123.2395128. \[18\] Sidney K. D’Mello and Arthur C. Graesser. “Confusion”. In: International Handbook of Emotions in Education. Ed. by Reinhard Pekrun and Lisa LinnenbrinkGarcia. New York: Routledge, 2014, pp. 289–310. \[19\] Sidney K. D’Mello and Arthur C. Graesser. “Malleability of Students’ Perceptions of an AffectSensitive Tutor and Its Influence on Learning”. In: Proceedings of the Twenty-Fifth International FLAIRS Conference. Ed. by G. Youngblood and P. McCarthy. Menlo Park: AAAI Press, 2012, pp. 432–437. \[20\] Sidney K. D’Mello and Jacqueline M. Kory. “A Review and Meta-Analysis of Multimodal Affect Detection Systems”. In: ACM Computing Surveys 47.3 (2015), pp. 1–36. ISSN: 0360-0300. DOI: 10.1145/ 2682899. \[21\] Sidney K. D’Mello, Blair Lehman, Jeremiah Sullins, Rosaire Daigle, Rebekah Combs, Kimberly Vogt, Lydia Perkins, and Arthur C. Graesser. “A Time for Emoting: When Affect-Sensitivity Is and Isn’t Effective at Promoting Deep Learning”. In: Proceedings of the 10th International Conference on Intelligent Tutoring Systems. Ed. by Vincent Aleven, Judy Kay, and Jack Mostow. Berlin, Heidelberg: Springer, 2010, pp. 245– 254. DOI: 10.1007/978-3-642-13388-6_29. \[22\] Sidney K. D’Mello and Caitlin Mills. “Emotions While Writing about Emotional and Non-Emotional Topics”. In: Motivation and Emotion 38.1 (Feb. 2014), pp. 140–156. DOI: 10.1007/s11031-013-9358- 1. \[23\] Charles Darwin. The Expression of the Emotions in Man and Animals. London: John Murray, 1872. DOI: 10.1037/10001-000. \[24\] John Dillon, Nigel Bosch, Malolan Chetlur, Nirandika Wanigasekara, G. Alex Ambrose, Bikram Sengupta, and Sidney K. D’Mello. “Student Emotion, Co-Occurrence, and Dropout in a MOOC Context.” In: Proceedings of the 9th International Conference on Educational Data Mining. International Educational Data Mining Society, 2016, pp. 353–357. \[25\] Paul Ekman and Wallace Friesen. The Facial Action Coding System: A Technique for the Measurement of Facial Movement. Palo Alto: Consulting Psychologists Press, 1978. \[26\] Emotient. “FACET: Facial Expression Recognition Software”. In: (2014). \[27\] Camille A. Farrington, Melissa Roderick, Elaine Allensworth, Jenny Nagaoka, Tasha Seneca Keyes, David W. Johnson, and Nicole O. Beechum. Teaching Adolescents to Become Learners: The Role of Noncognitive Factors in Shaping School Performance: A Critical Literature Review. Chicago: University of Chicago Consortium on Chicago School Research, 2012. \[28\] Kate Forbes-Riley and Diane Litman. “Benefits and Challenges of Real-Time Uncertainty Detection and Adaptation in a Spoken Dialogue Computer Tutor”. In: Speech Communication 53.9-10 (Nov. 2011), pp. 1115–1136. DOI: 10.1016/j.specom.2011. 02.006. \[29\] Barbara L. Fredrickson and Christine Branigan. “Positive Emotions Broaden the Scope of Attention and Thought-action Repertoires”. In: Cognition & Emotion 19.3 (Mar. 2005), pp. 313–332. DOI: 10 . 1080/02699930441000238. \[30\] Brian M. Galla, Benjamin D. Plummer, Rachel E. White, David Meketon, Sidney K. D’Mello, and Angela L. Duckworth. “The Academic Diligence Task (ADT): Assessing Individual Differences in Effort on Tedious but Important Schoolwork”. In: Contemporary Educational Psychology 39.4 (2014), pp. 314– 325. DOI: 10.1016/j.cedpsych.2014.08.001. \[31\] Arthur C. Graesser, P. Chipman, B.C. Haynes, and Andrew M. Olney. “AutoTutor: An Intelligent Tutoring System With Mixed-Initiative Dialogue”. In: IEEE Transactions on Education 48.4 (Nov. 2005), pp. 612–618. DOI: 10.1109/TE.2005.856149. \[32\] Beate Grawemeyer, Manolis Mavrikis, Wayne Holmes, Sergio Gutiérrez-Santos, Michael Wiedmann, and Nikol Rummel. “Affective Learning: Improving Engagement and Enhancing Learning with Affect-Aware Feedback”. In: User Modeling and UserAdapted Interaction 27.1 (Mar. 2017), pp. 119–158. DOI: 10.1007/s11257-017-9188-z. \[33\] James J Gross. “Emotion Regulation.” In: Handbook of Emotions, 3rd Ed. New York: The Guilford Press, 2008, pp. 497–512. ISBN: 1-59385-650-4 (Hardcover); 978-1-59385-650-2 (Hardcover). \[34\] Ursula Hess. “Now You See It, Now You Don’t– the Confusing Case of Confusion as an Emotion: Commentary on Rozin and Cohen (2003).” In: Emotion 3.1 (2003), pp. 76–80. DOI: 10.1037/1528- 3542.3.1.76. \[35\] Stephen Hutt, Joseph F. Grafsgaard, and Sidney K. D’Mello. “Time to Scale : Generalizable Affect Detection for Tens of Thousands of Students across an Entire School Year”. In: 2019 CHI Conference on Human Factors in Computing Systems Proceedings (CHI 2019). ACM Press, 2019. DOI: 10.1145/3290607. 3300726. \[36\] Stephen Hutt, Kristina Krasich, Caitlin Mills, Nigel Bosch, Shelby White, James R. Brockmole, and Sidney K. D’Mello. “Automated Gaze-Based Mind Wandering Detection during Computerized Learning in Classrooms”. In: User Modeling and UserAdapted Interaction (2019), pp. 1–35. DOI: 10.1007/ s11257-019-09228-5. \[37\] Alice M. Isen. “Some Ways in Which Positive Affect Influences Decision Making and Problem Solving.” In: Handbook of Emotions, 3rd Ed. New York, NY, US: The Guilford Press, 2008, pp. 548–573. ISBN: 1-59385- 650-4 (Hardcover); 978-1-59385-650-2 (Hardcover). \[38\] Carroll E. Izard. “The Many Meanings/Aspects of Emotion: Definitions, Functions, Activation, and Regulation”. In: Emotion Review 2.4 (Oct. 2010), pp. 363–370. DOI: 10.1177/1754073910374661. \[39\] Sandeep M. Jayaprakash, Erik W. Moody, Eitel J. M. Lauría, James R. Regan, and Joshua D. Baron. “Early Alert of Academically At-Risk Students: An Open Source Analytics Initiative”. In: Journal of Learning Analytics 1.1 (2014), pp. 6–47. \[40\] Emily Jensen, Meghan Dale, Patrick J. Donnelly, Cathlyn Stone, Sean Kelly, Amanda Godley, and Sidney K. D’Mello. “Toward Automated Feedback on Teacher Discourse to Enhance Teacher Learning”. In: 2020 CHI Conference on Human Factors in Computing Systems Proceedings (CHI 2020). ACM Press, 2020. ISBN: 978-1-4503-6708-0. DOI: 10.1145/3313831. 3376418. \[41\] Emily Jensen, Stephen Hutt, and Sidney K. D’Mello. “Generalizability of Sensor-Free Affect Detection Models in a Longitudinal Dataset of Tens of Thousands of Students”. In: The 12th International Conference on Educational Data Mining. Ed. by Michel Desmarais, Collin F. Lynch, Agathe Merceron, and Roger Nkambou. 2019, pp. 324–329. \[42\] Shiming Kai, Luc Paquette, Ryan S.J.d. Baker, Nigel Bosch, Sidney K. D’Mello, Jaclyn Ocumpaugh, Valerie J. Shute, and Matthew Ventura. “Comparison of Face-Based and Interaction-Based Affect Detectors in Physics Playground”. In: Proceedings of the 8th International Conference on Educational Data Mining. International Educational Data Mining Society, 2015, pp. 77–84. \[43\] Shamya Karumbaiah, Ryan S.J.d. Baker, and Valerie J. Shute. “Predicting Quitting in Students Playing a Learning Game”. In: Proceedings of the 11th International Conference on Educational Data Mining, EDM 2018. International Educational Data Mining Society, 2018. \[44\] Jacqueline Kory, Sidney K. D’Mello, and Andrew M. Olney. “Motion Tracker: Camera-Based Monitoring of Bodily Movements Using Motion Silhouettes”. In: PLOS ONE 10.6 (June 2015). Ed. by Philip Allen. DOI: 10.1371/journal.pone.0130293. \[45\] Blair Lehman, Sidney K. D’Mello, Amber Chauncey Strain, Melissa Gross, Allyson Dobbins, Patricia Wallace, Keith Millis, and Arthur C. Graesser. “Inducing and Tracking Confusion with Contradictions during Critical Thinking and Scientific Reasoning”. In: ed. by Gautam Biswas, Susan Bull, Judy Kay, and Antonija Mitrovic. Berlin: Springer Berlin Heidelberg, 2011, pp. 171–178. ISBN: 978-3- 642-21869-9. \[46\] Marc D. Lewis. “Bridging Emotion Theory and Neurobiology through Dynamic Systems Modeling”. In: Behavioral and Brain Sciences 28.2 (Apr. 2005), pp. 169–194. DOI: 10 . 1017 / S0140525X0500004X. \[47\] Christie McClendon, Robin Massey Neugebauer, and Amanda King. “Grit, Growth Mindset, and Deliberate Practice in Online Learning”. In: Journal of Instructional Research 6.1 (2017), pp. 8–17. DOI: 10.9743/jir.2017.2. \[48\] Gary J. McKeown and Ian Sneddon. “Modeling Continuous Self-Report Measures of Perceived Emotion Using Generalized Additive Mixed Models.” In: Psychological Methods 19.1 (2014), pp. 155– 174. DOI: 10.1037/a0034282. \[49\] Batja Mesquita and Michael Boiger. “Emotions in Context: A Sociodynamic Model of Emotions”. In: Emotion Review 6.4 (Oct. 2014), pp. 298–302. DOI: 10.1177/1754073914534480. \[50\] Agnes Moors. “Flavors of Appraisal Theories of Emotion”. In: Emotion Review 6.4 (Oct. 2014), pp. 303–307. DOI: 10.1177/1754073914534477. \[51\] Martin Nystrand, Adam Gamoran, Robert Kachur, and Catherine Prendergast. Opening Dialogue: Understanding the Dynamics of Language and Learning in the English Classroom. New York: Teachers College Press, 1997. \[52\] Jaclyn Ocumpaugh, Ryan S.J.d. Baker, and Ma. Mercedes T. Rodrigo. Baker-Rodrigo Observation Method Protocol (BROMP) 1.0. Training Manual Version 1.0. Tech. rep. New York, NY, 2012. \[53\] OECD. “PISA 2015 Collaborative Problem Solving Framework”. In: (2015). \[54\] A Ohman and J. J. Soares. “"Unconscious Anxiety": Phobic Responses to Masked Stimuli.” In: Journal of abnormal psychology 103.2 (May 1994), pp. 231–40. DOI: 10.1037//0021-843x.103.2.231. \[55\] Zach A. Pardos, Ryan S.J.d. Baker, Maria San Pedro, Sujith M. Gowda, and Supreeth M. Gowda. “Affective States and State Tests: Investigating How Affect and Engagement during the School Year Predict End-of-Year Learning Outcomes”. In: Journal of Learning Analytics 1.1 (2014), pp. 107–128. DOI: 10.18608/jla.2014.11.6. \[56\] Brian Parkinson, Agneta H Fischer, and Antony S R Manstead. Emotion in Social Relations: Cultural, Group, and Interpersonal Processes. Parkinson, Brian: Oxford University, Experimental Psychology, Oxford, United Kingdom: Psychology Press, 2005. ISBN: 1-84169-045-7 (Hardcover); 1-84169-046-5 (Paperback). \[57\] Reinhard Pekrun and Elizabeth J. Stephens. “Academic Emotions.” In: APA Educational Psychology Handbook, Vol 2: Individual Differences and Cultural and Contextual Factors. Washington: American Psychological Association, 2012, pp. 3–31. DOI: 10 . 1037/13274-001. \[58\] Megan J Price, Nicholas V Mudrick, Michelle Taub, and Roger Azevedo. “The Role of Negative Emotions and Emotion Regulation on Self-Regulated Learning with MetaTutor”. In: Proceedings of the International Conference on Intelligent Tutoring Systems. Ed. by Roger Nkambou, Roger Azevedo, and Julita Vassileva. Cham: Springer International Publishing, 2018, pp. 170–179. ISBN: 978-3-319-91464-0. \[59\] Anand Ramakrishnan, Erin Ottmar, Jennifer LoCasale-Crouch, and Jacob Whitehill. “Toward Automated Classroom Observation : Predicting Positive and Negative Climate”. In: IEEE Conference on Automatic Face and Gesture Recognition. 2019. ISBN: 978-1-72810-089-0. \[60\] Leena Razzaq, Mingyu Feng, Goss Nuzzo-Jones, Neil T. Heffernan, Kenneth Koedinger, Brian Junker, Steven Ritter, Andrea Knight, Edwin Mercado, Terrence E. Turner, Ruta Upalekar, Jason A. Walonoski, Michael A. Macasek, Christopher Aniszczyk, Sanket Choksey, Tom Livak, and Kai Rasmussen. “The Assistment Project: Blending Assessment and Assisting”. In: Proceedings of the 12th International Conference on Artificial Intelligence in Education. Ed. by C. Loi and G. McCalla. Amsterdam: IOS Press, 2005, pp. 555–562. \[61\] Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne. “Introducing the RECOLA Multimodal Corpus of Remote Collaborative and Affective Interactions”. In: 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG). IEEE, Apr. 2013, pp. 1–8. ISBN: 978-1-4673-5546-9. DOI: 10.1109/ FG.2013.6553805. \[62\] Erika L. Rosenberg. “Levels of Analysis and the Organization of Affect”. In: Review of General Psychology 2.3 (Sept. 1998), pp. 247–270. DOI: 10.1037/ 1089-2680.2.3.247. \[63\] Paul Rozin and Adam B. Cohen. “High Frequency of Facial Expressions Corresponding to Confusion, Concentration, and Worry in an Analysis of Naturally Occurring Facial Expressions of Americans.” In: Emotion (Washington, D.C.) 3.1 (Mar. 2003), pp. 68–75. DOI: 10.1037/1528-3542.3.1.68. \[64\] Jennifer Sabourin, Bradford Mott, and James C Lester. “Modeling Learner Affect with Theoretically Grounded Dynamic Bayesian Networks”. In: Proceedings of the 4th International Conference on Affective Computing and Intelligent Interaction. Ed. by Sidney K. D’Mello, Arthur C. Graesser, Björn Schuller, and Jean-Claude Martin. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 286–295. ISBN: 978-3- 642-24600-5. \[65\] Maria Ofelia Clarissa Z. San Pedro, Ryan S.J.d. Baker, Alex J. Bowers, and Neil T. Heffernan. “Predicting College Enrollment from Student Interaction with an Intelligent Tutoring System in Middle School”. In: Proceedings of the 6th International Conference on Educational Data Mining. Ed. by Sidney K. D’Mello, Rafael A. Calvo, and Andrew M. Olney. International Educational Data Mining Society, 2013, pp. 177–184. \[66\] Norbert Schwarz. “Feelings-as-Information Theory.” In: Handbook of Theories of Social Psychology, Vol. 1. Thousand Oaks, CA: Sage Publications Ltd, 2012, pp. 289–308. ISBN: 978-0-85702-960-7 (Hardcover). DOI: 10.4135/9781446249215.n15. \[67\] Valerie J. Shute, Matthew Ventura, and Yoon Jeon Kim. “Assessment and Learning of Qualitative Physics in Newton’s Playground”. In: The Journal of Educational Research 106.6 (Nov. 2013), pp. 423–430. DOI: 10.1080/00220671.2013.832970. \[68\] Amber Chauncey Strain and Sidney K. D’Mello. “Affect Regulation During Learning: The Enhancing Effect of Cognitive Reappraisal”. In: Applied Cognitive Psychology 29.1 (Jan. 2015), pp. 1–19. DOI: 10.1002/acp.3049. \[69\] Jessica L. Tracy. “An Evolutionary Approach to Understanding Distinct Emotions”. In: Emotion Review 6.4 (Oct. 2014), pp. 308–312. DOI: 10.1177/ 1754073914534478. \[70\] Virginia M. C. Tze, Lia M. Daniels, Erin Buhr, and Lily Le. “Affective Profiles in a Massive Open Online Course and Their Relationship with Engagement”. In: Frontiers in Education 2 (Dec. 2017). DOI: 10.3389/feduc.2017.00065. \[71\] Zuowei Wang, Kevin F. Miller, and Kai S. Cortina. “Using the LENA in Teacher Training: Promoting Student Involvement through Automated Feedback”. In: Unterrichtswissenshaft 4 (2013), pp. 290– 305. \[72\] Miaomiao Wen, Diyi Yang, and Carolyn P. Rosé. “Sentiment Analysis in MOOC Discussion Forums: What Does It Tell Us?” In: Proceedings of the 7th International Conference on Educational Data Mining. London: International Educational Data Mining Society, 2014, pp. 130–137. \[73\] Beverly Woolf, Winslow Burleson, Ivon Arroyo, Toby Dragon, David Cooper, and Rosalind Picard. “Affect-Aware Tutors: Recognising and Responding to Student Affect”. In: International Journal of Learning Technology 4.3/4 (2009), pp. 129–129. DOI: 10.1504/IJLT.2009.028804. \[74\] Wanli Xing, Hengtao Tang, and Bo Pei. “Beyond Positive and Negative Emotions: Looking into the Role of Achievement Emotions in Discussion Forums of MOOCs”. In: The Internet and Higher Education 43 (Oct. 2019). DOI: 10.1016/j.iheduc. 2019.100690. \[75\] Zhihong Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. “A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions”. In: IEEE Transactions on Pattern Analysis and Machine Intelligence 31.1 (Jan. 2009), pp. 39–58. DOI: 10.1109/TPAMI.2008.52.</Info>
</div>
